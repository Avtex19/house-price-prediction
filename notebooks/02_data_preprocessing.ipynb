{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Data Preprocessing â€” House Price Prediction (Option B)\n",
    "\n",
    "This notebook implements a reproducible preprocessing pipeline and writes processed datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T11:41:11.784184Z",
     "start_time": "2026-01-11T11:41:11.554617Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def remove_outliers(df: pd.DataFrame, columns: list, factor: float = 1.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes outliers from the dataframe using the IQR (Interquartile Range) method.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe.\n",
    "    columns : list\n",
    "        List of column names to check for outliers.\n",
    "    factor : float, optional\n",
    "        The IQR factor threshold (default is 1.5).\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with outliers removed.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - (factor * IQR)\n",
    "        upper_bound = Q3 + (factor * IQR)\n",
    "        \n",
    "        # Filter\n",
    "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "        \n",
    "    print(f\"Outlier removal: Reduced data from {len(df)} to {len(df_clean)} rows.\")\n",
    "    return df_clean\n",
    "\n",
    "def create_derived_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates new features from existing columns to improve model performance.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with added engineered features.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    # Feature engineering\n",
    "    df_out['BedroomsPerRoom'] = df_out['AveBedrms'] / df_out['AveRooms']\n",
    "    df_out['RoomsPerPerson'] = df_out['AveRooms'] / df_out['AveOccup']\n",
    "    df_out['RoomsMinusBedrooms'] = df_out['AveRooms'] - df_out['AveBedrms']\n",
    "    return df_out\n",
    "\n",
    "def preprocess_and_split(df: pd.DataFrame, target_col: str, test_size: float = 0.2):\n",
    "    \"\"\"\n",
    "    Splits data, scales features, and prepares train/test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataset.\n",
    "    target_col : str\n",
    "        The name of the target variable column.\n",
    "    test_size : float\n",
    "        Proportion of dataset to include in the test split.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    tuple\n",
    "        (X_train_df, X_test_df, y_train, y_test, numeric_features)\n",
    "    \"\"\"\n",
    "    feature_cols = [c for c in df.columns if c != target_col]\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[feature_cols], df[target_col], test_size=test_size, random_state=42\n",
    "    )\n",
    "\n",
    "    # Preprocessing pipeline\n",
    "    numeric_features = feature_cols\n",
    "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "    # Fit/Transform\n",
    "    X_train_proc = preprocessor.fit_transform(X_train)\n",
    "    X_test_proc = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    X_train_df = pd.DataFrame(X_train_proc, columns=numeric_features)\n",
    "    X_test_df = pd.DataFrame(X_test_proc, columns=numeric_features)\n",
    "    \n",
    "    return X_train_df, X_test_df, y_train, y_test, numeric_features\n",
    "\n",
    "# --- Execution ---\n",
    "# Paths\n",
    "raw_csv_path = Path('../data/raw/california_housing.csv').resolve()\n",
    "processed_dir = Path('../data/processed').resolve()\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv(raw_csv_path)\n",
    "target_column = 'MedHouseVal'\n",
    "base_features = [c for c in df.columns if c != target_column]\n",
    "\n",
    "# 2. Handle Missing (Imputation)\n",
    "df[base_features] = df[base_features].fillna(df[base_features].median())\n",
    "\n",
    "# 3. Detect and Remove Outliers (Added Requirement)\n",
    "# We focus on MedHouseVal and key income features for outlier removal\n",
    "cols_to_check = ['MedHouseVal', 'MedInc', 'AveRooms']\n",
    "df = remove_outliers(df, cols_to_check, factor=3.0) # Using conservative factor of 3.0\n",
    "\n",
    "# 4. Feature Engineering\n",
    "df = create_derived_features(df)\n",
    "\n",
    "# 5. Split and Scale\n",
    "X_train_df, X_test_df, y_train, y_test, feat_cols = preprocess_and_split(df, target_column)\n",
    "\n",
    "# 6. Save Processed Data\n",
    "train_out = processed_dir / 'train.csv'\n",
    "test_out = processed_dir / 'test.csv'\n",
    "\n",
    "pd.concat([X_train_df, y_train.reset_index(drop=True)], axis=1).to_csv(train_out, index=False)\n",
    "pd.concat([X_test_df, y_test.reset_index(drop=True)], axis=1).to_csv(test_out, index=False)\n",
    "\n",
    "print(f'Wrote processed files to: {processed_dir}')\n",
    "\n",
    "# 7. Save Config\n",
    "config = {\n",
    "    'target_column': target_column,\n",
    "    'feature_columns': feat_cols,\n",
    "    'engineered_features': ['BedroomsPerRoom', 'RoomsPerPerson', 'RoomsMinusBedrooms'],\n",
    "    'outlier_removal': 'IQR method (factor=3.0)',\n",
    "    'scaler': 'StandardScaler()',\n",
    "    'split': {'test_size': 0.2, 'random_state': 42}\n",
    "}\n",
    "with open(processed_dir / 'preprocessing_config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier removal: Reduced data from 20640 to 20319 rows.\n",
      "Wrote processed files to: /Users/atukaberadze/Desktop/final-data-science/data/processed\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T11:41:11.786230Z",
     "start_time": "2026-01-11T11:41:11.785116Z"
    }
   },
   "source": [
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
